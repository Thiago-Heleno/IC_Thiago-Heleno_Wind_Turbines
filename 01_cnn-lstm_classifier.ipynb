{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header_md",
      "metadata": {},
      "source": [
        "# Wind Turbine Failure Prediction - Improved Pipeline (v2)\n",
        "\n",
        "## Overview\n",
        "This is an improved version of the original pipeline. The critical change is the **prevention of data leakage**.\n",
        "\n",
        "## Key Improvements\n",
        "- **Correct Split Strategy**: Data is split into Train/Val/Test **before** any scaling or dimensionality reduction.\n",
        "- **No Look-Ahead Bias**: The Scaler and PCA are fitted ONLY on the Training set, and then applied to Validation and Test sets.\n",
        "- **Robust Evaluation**: Metrics reflect true generalization capability.\n",
        "\n",
        "## Pipeline Structure\n",
        "1. Configuration & Data Loading\n",
        "2. Data Cleaning & Labeling\n",
        "3. **Temporal Split (Raw Data)**\n",
        "4. **Feature Engineering (Fit on Train, Transform All)**\n",
        "5. Sequence Creation\n",
        "6. Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, average_precision_score, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hyperparameters\n",
        "REMOVE_OUTLIERS = False\n",
        "WINDOW_HOURS = 12\n",
        "USE_PCA = False\n",
        "PCA_VARIANCE = 0.95\n",
        "SEQ_LENGTH = 50\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 50\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "# Split Ratios\n",
        "TRAIN_RATIO = 0.6\n",
        "VAL_RATIO = 0.12\n",
        "# Test is remainder\n",
        "\n",
        "# Model Params\n",
        "FOCAL_ALPHA = 0.75\n",
        "FOCAL_GAMMA = 2.0\n",
        "LR_CNN_LSTM = 0.0001\n",
        "LR_TCN = 0.0001\n",
        "LR_TRANSFORMER = 0.0001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "DROPOUT = 0.45\n",
        "\n",
        "USE_WEIGHTED_SAMPLER = True\n",
        "USE_CLASS_WEIGHTS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data_load_md",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "scada = pd.read_csv(\"Datasets/SCADA/EDP-SCADA-2017.csv\")\n",
        "failure = pd.read_csv(\"Datasets/SCADA/EDP-Failure-2017.csv\")\n",
        "\n",
        "# Timestamp Conversion\n",
        "scada['Timestamp'] = pd.to_datetime(scada['Timestamp'])\n",
        "failure['Timestamp'] = pd.to_datetime(failure['Timestamp'])\n",
        "\n",
        "# Sort\n",
        "scada = scada.sort_values(['Turbine_ID', 'Timestamp']).reset_index(drop=True)\n",
        "failure = failure.sort_values(['Turbine_ID', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# Clean numeric columns\n",
        "scada_cols = [col for col in scada.columns if col not in ['Timestamp', 'Turbine_ID']]\n",
        "for col in scada_cols:\n",
        "    scada[col] = pd.to_numeric(scada[col], errors='coerce')\n",
        "\n",
        "# Simple Imputation (Median of whole dataset is acceptable for initial cleaning of sensor errors, \n",
        "# though ideally should be done per split. Keeping simple here as NaNs are rare)\n",
        "scada[scada_cols] = scada[scada_cols].fillna(scada[scada_cols].median())\n",
        "\n",
        "print(f\"Data Loaded: {scada.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "labeling",
      "metadata": {},
      "outputs": [],
      "source": [
        "def label_data(scada_df, failure_df, window_hours=48):\n",
        "    scada_labeled = scada_df.copy()\n",
        "    scada_labeled['Label'] = 0\n",
        "    \n",
        "    for _, fail_row in failure_df.iterrows():\n",
        "        turbine = fail_row['Turbine_ID']\n",
        "        fail_time = fail_row['Timestamp']\n",
        "        \n",
        "        mask = (\n",
        "            (scada_labeled['Turbine_ID'] == turbine) &\n",
        "            (scada_labeled['Timestamp'] <= fail_time) &\n",
        "            (scada_labeled['Timestamp'] >= fail_time - pd.Timedelta(hours=window_hours))\n",
        "        )\n",
        "        scada_labeled.loc[mask, 'Label'] = 1\n",
        "    \n",
        "    return scada_labeled\n",
        "\n",
        "scada_labeled = label_data(scada, failure, WINDOW_HOURS)\n",
        "print(f\"Failure Samples: {scada_labeled['Label'].sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split_md",
      "metadata": {},
      "source": [
        "## 2. Temporal Split (The Fix)\n",
        "Here we split the data **before** scaling or PCA. This ensures no information from the validation/test sets leaks into the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split_logic",
      "metadata": {},
      "outputs": [],
      "source": [
        "def temporal_split_raw_data(df, train_ratio=0.6, val_ratio=0.12):\n",
        "    train_dfs = []\n",
        "    val_dfs = []\n",
        "    test_dfs = []\n",
        "    \n",
        "    # Split per turbine to maintain chronology\n",
        "    for turbine in df['Turbine_ID'].unique():\n",
        "        turbine_df = df[df['Turbine_ID'] == turbine].copy()\n",
        "        n = len(turbine_df)\n",
        "        \n",
        "        train_end = int(n * train_ratio)\n",
        "        val_end = int(n * (train_ratio + val_ratio))\n",
        "        \n",
        "        train_dfs.append(turbine_df.iloc[:train_end])\n",
        "        val_dfs.append(turbine_df.iloc[train_end:val_end])\n",
        "        test_dfs.append(turbine_df.iloc[val_end:])\n",
        "    \n",
        "    return pd.concat(train_dfs), pd.concat(val_dfs), pd.concat(test_dfs)\n",
        "\n",
        "train_df, val_df, test_df = temporal_split_raw_data(scada_labeled, TRAIN_RATIO, VAL_RATIO)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Val shape:   {val_df.shape}\")\n",
        "print(f\"Test shape:  {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature_eng_md",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering (Fit on Train, Transform All)\n",
        "We fit the Scaler and PCA only on `train_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature_eng",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_cols = [col for col in scada_cols]\n",
        "\n",
        "# 1. Scaling\n",
        "scaler = StandardScaler()\n",
        "# FIT only on Train\n",
        "X_train_scaled = scaler.fit_transform(train_df[feature_cols])\n",
        "# TRANSFORM Val and Test\n",
        "X_val_scaled = scaler.transform(val_df[feature_cols])\n",
        "X_test_scaled = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "# 2. PCA (Conditional)\n",
        "if USE_PCA:\n",
        "    pca = PCA(n_components=PCA_VARIANCE, random_state=RANDOM_SEED)\n",
        "    # FIT only on Train\n",
        "    X_train_final = pca.fit_transform(X_train_scaled)\n",
        "    # TRANSFORM Val and Test\n",
        "    X_val_final = pca.transform(X_val_scaled)\n",
        "    X_test_final = pca.transform(X_test_scaled)\n",
        "    print(f\"PCA Enabled: {X_train_scaled.shape[1]} features -> {pca.n_components_} components\")\n",
        "else:\n",
        "    X_train_final = X_train_scaled\n",
        "    X_val_final = X_val_scaled\n",
        "    X_test_final = X_test_scaled\n",
        "    print(f\"PCA Disabled: Using all {X_train_scaled.shape[1]} features\")\n",
        "\n",
        "# Get labels and turbine IDs for sequence generation\n",
        "y_train = train_df['Label'].values\n",
        "y_val = val_df['Label'].values\n",
        "y_test = test_df['Label'].values\n",
        "\n",
        "turbines_train = train_df['Turbine_ID'].values\n",
        "turbines_val = val_df['Turbine_ID'].values\n",
        "turbines_test = test_df['Turbine_ID'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sequences",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(X, y, turbine_ids, seq_length=50):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    \n",
        "    unique_turbines = np.unique(turbine_ids)\n",
        "    for turbine in unique_turbines:\n",
        "        turbine_mask = turbine_ids == turbine\n",
        "        X_turbine = X[turbine_mask]\n",
        "        y_turbine = y[turbine_mask]\n",
        "        \n",
        "        if len(X_turbine) < seq_length:\n",
        "            continue\n",
        "            \n",
        "        for i in range(len(X_turbine) - seq_length + 1):\n",
        "            sequences.append(X_turbine[i:i+seq_length])\n",
        "            labels.append(y_turbine[i+seq_length-1])\n",
        "            \n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "print(\"Creating sequences...\")\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_final, y_train, turbines_train, SEQ_LENGTH)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_final, y_val, turbines_val, SEQ_LENGTH)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_final, y_test, turbines_test, SEQ_LENGTH)\n",
        "\n",
        "print(f\"Train Sequences: {X_train_seq.shape}\")\n",
        "print(f\"Val Sequences:   {X_val_seq.shape}\")\n",
        "print(f\"Test Sequences:  {X_test_seq.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataloaders",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train_seq, y_train_seq)\n",
        "val_dataset = TimeSeriesDataset(X_val_seq, y_val_seq)\n",
        "test_dataset = TimeSeriesDataset(X_test_seq, y_test_seq)\n",
        "\n",
        "# Weighted Sampler for Train\n",
        "class_counts = np.bincount(y_train_seq)\n",
        "class_weights = 1. / class_counts\n",
        "sample_weights = class_weights[y_train_seq]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model_def",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, seq_length, num_classes=2):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout1 = nn.Dropout(DROPOUT)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=2, \n",
        "                           batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
        "        \n",
        "        self.fc1 = nn.Linear(256, 64)\n",
        "        self.dropout2 = nn.Dropout(DROPOUT)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = lstm_out[:, -1, :]\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.0, class_weights=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss(reduction='none', weight=self.class_weights)(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_func",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device='cuda', patience=10):\n",
        "    model.to(device)\n",
        "    best_val_f1 = 0.0\n",
        "    best_model_state = None\n",
        "    epochs_without_improvement = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "                val_preds.extend(preds)\n",
        "                val_labels.extend(y_batch.cpu().numpy())\n",
        "        \n",
        "        val_f1 = f1_score(val_labels, val_preds, average='binary', zero_division=0)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss/len(train_loader):.4f} | Val F1: {val_f1:.4f}\")\n",
        "        \n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            \n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "            \n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_dim = X_train_final.shape[1]\n",
        "\n",
        "# Class Weights for Loss\n",
        "class_counts_train = np.bincount(y_train_seq)\n",
        "class_weights_values = len(y_train_seq) / (len(class_counts_train) * class_counts_train)\n",
        "class_weights_tensor = torch.FloatTensor(class_weights_values).to(device)\n",
        "\n",
        "model = CNN_LSTM(input_dim=input_dim, seq_length=SEQ_LENGTH)\n",
        "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, class_weights=class_weights_tensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR_CNN_LSTM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "model = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
        "                    num_epochs=NUM_EPOCHS, device=device, patience=EARLY_STOPPING_PATIENCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y_batch.numpy())\n",
        "    \n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "    print(f\"F1 Score: {f1_score(all_labels, all_preds):.4f}\")\n",
        "    print(f\"Precision: {precision_score(all_labels, all_preds):.4f}\")\n",
        "    print(f\"Recall: {recall_score(all_labels, all_preds):.4f}\")\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "evaluate(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}