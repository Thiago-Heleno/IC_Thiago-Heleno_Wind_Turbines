{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45b2d41",
   "metadata": {},
   "source": [
    "# Wind Turbine Failure Prediction using Deep Learning\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive deep learning pipeline for predicting failures in wind turbines using SCADA sensor data. The project explores three temporal modeling architectures: CNN-LSTM, TCN, and Transformer.\n",
    "\n",
    "## Dataset\n",
    "- **SCADA Data**: Time-series sensor measurements (10-minute intervals)\n",
    "- **Failure Data**: Logged failure events with timestamps and component information\n",
    "- **Source**: EDP 2017 dataset\n",
    "\n",
    "## Pipeline Structure\n",
    "1. Configuration and Data Loading\n",
    "2. Data Cleaning and Preprocessing\n",
    "3. Feature Engineering (PCA, Temporal Sequences)\n",
    "4. Temporal Train/Val/Test Split\n",
    "5. Model Definition and Training\n",
    "6. Evaluation and Comparison\n",
    "\n",
    "## Key Features\n",
    "- Temporal split (no data leakage)\n",
    "- Class balancing (WeightedSampler + Focal Loss + Class Weights)\n",
    "- Early stopping with best model restoration\n",
    "- Comprehensive evaluation metrics (F1, Precision, Recall, ROC-AUC, PR-AUC, MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a23f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data preprocessing\n",
    "REMOVE_OUTLIERS = False  # Enable/disable outlier removal using IQR method\n",
    "\n",
    "# Failure labeling window (hours before failure event)\n",
    "# 48h provides balance: enough pre-failure samples without excessive false positives\n",
    "# Trade-off: Larger window = more positive samples but less precise timing\n",
    "WINDOW_HOURS = 48\n",
    "\n",
    "# PCA variance threshold for dimensionality reduction\n",
    "# 0.95 retains 95% of feature variance while reducing computational cost\n",
    "PCA_VARIANCE = 0.95\n",
    "\n",
    "# Sequence length for temporal models (number of timesteps)\n",
    "# 50 timesteps × 10 min/timestep = ~8.3 hours of historical data\n",
    "# Captures medium-term temporal patterns without excessive memory usage\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# Temporal split ratios (train/val/test)\n",
    "# 60/12/28 split: Small validation set due to limited failure samples\n",
    "# Maintains chronological order within each turbine (no data leakage)\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.12\n",
    "\n",
    "# Focal Loss parameters for imbalanced classification\n",
    "# Alpha=0.75: Weights minority class (failures) more heavily\n",
    "# Gamma=2.0: Focuses learning on hard-to-classify examples\n",
    "FOCAL_ALPHA = 0.75\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "# Learning rates per model\n",
    "LR_CNN_LSTM = 0.0001\n",
    "LR_TCN = 0.0001\n",
    "LR_TRANSFORMER = 0.0001\n",
    "\n",
    "# Regularization\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Class balancing strategies\n",
    "USE_WEIGHTED_SAMPLER = True  # Oversample minority class during training\n",
    "USE_CLASS_WEIGHTS = True     # Weight loss by inverse class frequency\n",
    "\n",
    "# Model architecture hyperparameters\n",
    "CNN_LSTM_FILTERS = [64, 128]\n",
    "CNN_LSTM_LSTM_HIDDEN = 128\n",
    "CNN_LSTM_LSTM_LAYERS = 2\n",
    "\n",
    "TCN_CHANNELS = [64, 128, 256]\n",
    "TCN_KERNEL_SIZE = 3\n",
    "\n",
    "TRANSFORMER_D_MODEL = 128\n",
    "TRANSFORMER_NHEAD = 8\n",
    "TRANSFORMER_LAYERS = 3\n",
    "\n",
    "# Dropout rate (applied to all models for regularization)\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c98eb1",
   "metadata": {},
   "source": [
    "## 1. Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b6505ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, average_precision_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c24c70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0d8b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCADA Data Loaded: 209,236 rows × 83 columns\n",
      "Columns: Turbine_ID, Timestamp, Gen_RPM_Max, Gen_RPM_Min, Gen_RPM_Avg... (showing first 5)\n",
      "Time Range: 2017-01-01T00:00:00+00:00 to 2017-12-31T23:50:00+00:00\n"
     ]
    }
   ],
   "source": [
    "scada = pd.read_csv(\"Datasets/SCADA/EDP-SCADA-2017.csv\")\n",
    "print(f\"SCADA Data Loaded: {scada.shape[0]:,} rows × {scada.shape[1]} columns\")\n",
    "print(f\"Columns: {', '.join(scada.columns[:5])}... (showing first 5)\")\n",
    "print(f\"Time Range: {scada['Timestamp'].min()} to {scada['Timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be251899",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc2f655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Data Loaded: 12 failure events\n",
      "Affected Turbines: 5\n",
      "Components: TRANSFORMER, GEARBOX, HYDRAULIC_GROUP, GENERATOR_BEARING, GENERATOR\n"
     ]
    }
   ],
   "source": [
    "failure = pd.read_csv(\"Datasets/SCADA/EDP-Failure-2017.csv\")\n",
    "print(f\"Failure Data Loaded: {failure.shape[0]} failure events\")\n",
    "print(f\"Affected Turbines: {failure['Turbine_ID'].nunique()}\")\n",
    "print(f\"Components: {', '.join(failure['Component'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da44cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada['Timestamp'] = pd.to_datetime(scada['Timestamp'])\n",
    "failure['Timestamp'] = pd.to_datetime(failure['Timestamp'])\n",
    "scada = scada.sort_values(['Turbine_ID', 'Timestamp']).reset_index(drop=True)\n",
    "failure = failure.sort_values(['Turbine_ID', 'Timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d7ed660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier removal disabled - using all data\n"
     ]
    }
   ],
   "source": [
    "scada_cols = [col for col in scada.columns if col not in ['Timestamp', 'Turbine_ID']]\n",
    "for col in scada_cols:\n",
    "    scada[col] = pd.to_numeric(scada[col], errors='coerce')\n",
    "\n",
    "if REMOVE_OUTLIERS:\n",
    "    Q1 = scada[scada_cols].quantile(0.01)\n",
    "    Q3 = scada[scada_cols].quantile(0.99)\n",
    "    IQR = Q3 - Q1\n",
    "    scada_clean = scada[~((scada[scada_cols] < (Q1 - 1.5 * IQR)) | (scada[scada_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    print(f\"Outliers removed: {len(scada) - len(scada_clean):,} samples\")\n",
    "else:\n",
    "    scada_clean = scada.copy()\n",
    "    print(\"Outlier removal disabled - using all data\")\n",
    "\n",
    "scada_clean[scada_cols] = scada_clean[scada_cols].fillna(scada_clean[scada_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6f5b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned: 209,236 samples, 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "nan_count = scada_clean[scada_cols].isnull().sum().sum()\n",
    "assert nan_count == 0, f\"Data cleaning failed: {nan_count} NaN values remaining\"\n",
    "print(f\"Data cleaned: {len(scada_clean):,} samples, 0 NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4a3314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET INFORMATION\n",
      "============================================================\n",
      "\n",
      "Temporal Coverage:\n",
      "  Start: 2017-01-01 00:00:00+00:00\n",
      "  End: 2017-12-31 23:50:00+00:00\n",
      "  Duration: 364 days\n",
      "\n",
      "Turbines:\n",
      "  Unique turbines: 4\n",
      "  Turbine IDs: ['T01', 'T06', 'T07', 'T11']\n",
      "\n",
      "Sampling Frequency:\n",
      "  Median interval: 0 days 00:10:00\n",
      "  Frequency: ~10 minutes\n",
      "\n",
      "Features:\n",
      "  Total features: 81\n",
      "  Feature names: Gen_RPM_Max, Gen_RPM_Min, Gen_RPM_Avg, Gen_RPM_Std, Gen_Bear_Temp_Avg... (showing first 5)\n",
      "\n",
      "Data Quality:\n",
      "  Total samples: 209,236\n",
      "  Samples per turbine: 52,309 (avg)\n",
      "  Missing values: 0 (after cleaning)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset Information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temporal coverage\n",
    "print(f\"\\nTemporal Coverage:\")\n",
    "print(f\"  Start: {scada_clean['Timestamp'].min()}\")\n",
    "print(f\"  End: {scada_clean['Timestamp'].max()}\")\n",
    "print(f\"  Duration: {(scada_clean['Timestamp'].max() - scada_clean['Timestamp'].min()).days} days\")\n",
    "\n",
    "# Turbine information\n",
    "print(f\"\\nTurbines:\")\n",
    "print(f\"  Unique turbines: {scada_clean['Turbine_ID'].nunique()}\")\n",
    "print(f\"  Turbine IDs: {sorted(scada_clean['Turbine_ID'].unique())}\")\n",
    "\n",
    "# Sampling frequency\n",
    "time_diffs = scada_clean.groupby('Turbine_ID')['Timestamp'].diff().dropna()\n",
    "median_interval = time_diffs.median()\n",
    "print(f\"\\nSampling Frequency:\")\n",
    "print(f\"  Median interval: {median_interval}\")\n",
    "print(f\"  Frequency: ~{int(median_interval.total_seconds() / 60)} minutes\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeatures:\")\n",
    "print(f\"  Total features: {len(scada_cols)}\")\n",
    "print(f\"  Feature names: {', '.join(scada_cols[:5])}... (showing first 5)\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  Total samples: {len(scada_clean):,}\")\n",
    "print(f\"  Samples per turbine: {len(scada_clean) // scada_clean['Turbine_ID'].nunique():,} (avg)\")\n",
    "print(f\"  Missing values: 0 (after cleaning)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c9c28",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77c364cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution:\n",
      "  Normal samples: 206,743\n",
      "  Failure samples: 2,493\n",
      "  Total samples: 209,236\n",
      "  Failure percentage: 1.19%\n",
      "  Imbalance ratio: 1:82 (Normal:Failure)\n"
     ]
    }
   ],
   "source": [
    "def label_data(scada_df, failure_df, window_hours=WINDOW_HOURS):\n",
    "    scada_labeled = scada_df.copy()\n",
    "    scada_labeled['Label'] = 0\n",
    "    scada_labeled['Component'] = 'NORMAL'\n",
    "    \n",
    "    for _, fail_row in failure_df.iterrows():\n",
    "        turbine = fail_row['Turbine_ID']\n",
    "        fail_time = fail_row['Timestamp']\n",
    "        component = fail_row['Component']\n",
    "        \n",
    "        mask = (\n",
    "            (scada_labeled['Turbine_ID'] == turbine) &\n",
    "            (scada_labeled['Timestamp'] <= fail_time) &\n",
    "            (scada_labeled['Timestamp'] >= fail_time - pd.Timedelta(hours=window_hours))\n",
    "        )\n",
    "        scada_labeled.loc[mask, 'Label'] = 1\n",
    "        scada_labeled.loc[mask, 'Component'] = component\n",
    "    \n",
    "    return scada_labeled\n",
    "\n",
    "scada_labeled = label_data(scada_clean, failure)\n",
    "\n",
    "normal_count = (scada_labeled['Label']==0).sum()\n",
    "failure_count = (scada_labeled['Label']==1).sum()\n",
    "total_count = len(scada_labeled)\n",
    "failure_pct = failure_count / total_count * 100\n",
    "imbalance_ratio = normal_count / failure_count\n",
    "\n",
    "print(f\"Data Distribution:\")\n",
    "print(f\"  Normal samples: {normal_count:,}\")\n",
    "print(f\"  Failure samples: {failure_count:,}\")\n",
    "print(f\"  Total samples: {total_count:,}\")\n",
    "print(f\"  Failure percentage: {failure_pct:.2f}%\")\n",
    "print(f\"  Imbalance ratio: 1:{int(imbalance_ratio)} (Normal:Failure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41a486ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 81 features -> 16 components (variance=0.95)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in scada_labeled.columns if col not in ['Timestamp', 'Turbine_ID', 'Label', 'Component']]\n",
    "X = scada_labeled[feature_cols].values\n",
    "y = scada_labeled['Label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=PCA_VARIANCE, random_state=RANDOM_SEED)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"PCA: {X.shape[1]} features -> {X_pca.shape[1]} components (variance={PCA_VARIANCE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8964db7",
   "metadata": {},
   "source": [
    "## 4. Failure Labeling and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d6cd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(X_pca).any(), \"PCA output contains NaN values\"\n",
    "assert X_pca.shape[1] > 0, \"PCA resulted in zero components\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1731875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences: (209040, 50, 16), Labels: (209040,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(X, y, turbine_ids, seq_length=50):\n",
    "    \"\"\"\n",
    "    Create temporal sequences per turbine.\n",
    "    Data is already sorted by (Turbine_ID, Timestamp) from preprocessing.\n",
    "    Sequences are created in chronological order within each turbine.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    seq_turbine_ids = []\n",
    "    \n",
    "    unique_turbines = np.unique(turbine_ids)\n",
    "    for turbine in unique_turbines:\n",
    "        turbine_mask = turbine_ids == turbine\n",
    "        X_turbine = X[turbine_mask]\n",
    "        y_turbine = y[turbine_mask]\n",
    "        \n",
    "        # Sequences are built in timestamp order (already sorted)\n",
    "        for i in range(len(X_turbine) - seq_length + 1):\n",
    "            sequences.append(X_turbine[i:i+seq_length])\n",
    "            labels.append(y_turbine[i+seq_length-1])\n",
    "            seq_turbine_ids.append(turbine)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), np.array(seq_turbine_ids)\n",
    "\n",
    "turbine_ids_original = scada_labeled['Turbine_ID'].values\n",
    "X_seq, y_seq, turbine_ids = create_sequences(X_pca, y, turbine_ids_original, SEQ_LENGTH)\n",
    "print(f\"Sequences: {X_seq.shape}, Labels: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11480b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_seq.shape[1] == SEQ_LENGTH, f\"Sequence length mismatch: expected {SEQ_LENGTH}, got {X_seq.shape[1]}\"\n",
    "assert len(X_seq) == len(y_seq), \"Sequences and labels length mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a9c1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal split: Train=60%, Val=12%, Test=28%\n",
      "Train: (125423, 50, 16), Val: (25084, 50, 16), Test: (58533, 50, 16)\n",
      "Train: 124847 normal, 576 failures (0.46%)\n",
      "Val: 23743 normal, 1341 failures (5.35%)\n",
      "Test: 57957 normal, 576 failures (0.98%)\n"
     ]
    }
   ],
   "source": [
    "def temporal_train_test_split(X, y, turbine_ids, train_ratio=0.7, val_ratio=0.15):\n",
    "    X_train_list, y_train_list = [], []\n",
    "    X_val_list, y_val_list = [], []\n",
    "    X_test_list, y_test_list = [], []\n",
    "    \n",
    "    unique_turbines = np.unique(turbine_ids)\n",
    "    for turbine in unique_turbines:\n",
    "        turbine_mask = turbine_ids == turbine\n",
    "        X_turbine = X[turbine_mask]\n",
    "        y_turbine = y[turbine_mask]\n",
    "        \n",
    "        n_samples = len(X_turbine)\n",
    "        train_idx = int(n_samples * train_ratio)\n",
    "        val_idx = int(n_samples * (train_ratio + val_ratio))\n",
    "        \n",
    "        X_train_list.append(X_turbine[:train_idx])\n",
    "        y_train_list.append(y_turbine[:train_idx])\n",
    "        \n",
    "        X_val_list.append(X_turbine[train_idx:val_idx])\n",
    "        y_val_list.append(y_turbine[train_idx:val_idx])\n",
    "        \n",
    "        X_test_list.append(X_turbine[val_idx:])\n",
    "        y_test_list.append(y_turbine[val_idx:])\n",
    "    \n",
    "    return (np.vstack(X_train_list), np.concatenate(y_train_list),\n",
    "            np.vstack(X_val_list), np.concatenate(y_val_list),\n",
    "            np.vstack(X_test_list), np.concatenate(y_test_list))\n",
    "\n",
    "X_train_temp, y_train_temp, X_val_temp, y_val_temp, X_test_temp, y_test_temp = temporal_train_test_split(\n",
    "    X_seq, y_seq, turbine_ids, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO\n",
    ")\n",
    "\n",
    "test_ratio = 1 - TRAIN_RATIO - VAL_RATIO\n",
    "print(f\"Temporal split: Train={TRAIN_RATIO:.0%}, Val={VAL_RATIO:.0%}, Test={test_ratio:.0%}\")\n",
    "print(f\"Train: {X_train_temp.shape}, Val: {X_val_temp.shape}, Test: {X_test_temp.shape}\")\n",
    "print(f\"Train: {(y_train_temp==0).sum()} normal, {(y_train_temp==1).sum()} failures ({(y_train_temp==1).sum()/len(y_train_temp)*100:.2f}%)\")\n",
    "print(f\"Val: {(y_val_temp==0).sum()} normal, {(y_val_temp==1).sum()} failures ({(y_val_temp==1).sum()/len(y_val_temp)*100:.2f}%)\")\n",
    "print(f\"Test: {(y_test_temp==0).sum()} normal, {(y_test_temp==1).sum()} failures ({(y_test_temp==1).sum()/len(y_test_temp)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8d340",
   "metadata": {},
   "source": [
    "## 5. Temporal Data Split and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "209eadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_failures = (y_train_temp==1).sum()\n",
    "val_failures = (y_val_temp==1).sum()\n",
    "test_failures = (y_test_temp==1).sum()\n",
    "\n",
    "if val_failures == 0 or test_failures == 0:\n",
    "    print(f\"\\nWARNING: Val or Test set has no failures!\")\n",
    "    print(f\"  Consider increasing WINDOW_HOURS or adjusting split ratios\")\n",
    "elif val_failures < 50 or test_failures < 50:\n",
    "    print(f\"\\nWARNING: Low failure count in validation/test sets\")\n",
    "    print(f\"  Metrics may be unstable with few failure samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a8e5e",
   "metadata": {},
   "source": [
    "### Analysis: Validation Set Failure Distribution\n",
    "\n",
    "The validation set shows higher failure concentration (9.26%) compared to train (1.05%) and test (0.98%). This is due to:\n",
    "\n",
    "1. **Temporal Characteristics**: Failures may cluster in specific time periods\n",
    "2. **Per-Turbine Variability**: Some turbines may have concentrated failures during validation window\n",
    "3. **WINDOW_HOURS Effect**: 48h labeling window affects different splits differently\n",
    "\n",
    "This distribution reflects realistic temporal patterns and does **not** indicate data leakage. The temporal split maintains chronological order within each turbine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "130c34a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Turbine Failure Rate Analysis:\n",
      "Turbine    Train%     Val%       Test%     \n",
      "----------------------------------------\n",
      "TT01           0.00%      4.60%      0.00%\n",
      "TT06           0.00%      4.53%      1.97%\n",
      "TT07           0.92%      7.72%      1.97%  <-- High Val\n",
      "TT11           0.92%      4.54%      0.00%\n"
     ]
    }
   ],
   "source": [
    "# Per-turbine failure analysis across splits\n",
    "unique_turbines = np.unique(turbine_ids)\n",
    "print(\"\\nPer-Turbine Failure Rate Analysis:\")\n",
    "print(f\"{'Turbine':<10} {'Train%':<10} {'Val%':<10} {'Test%':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for turbine in unique_turbines:\n",
    "    turbine_mask = turbine_ids == turbine\n",
    "    X_turbine = X_seq[turbine_mask]\n",
    "    y_turbine = y_seq[turbine_mask]\n",
    "    \n",
    "    n_samples = len(X_turbine)\n",
    "    train_idx = int(n_samples * TRAIN_RATIO)\n",
    "    val_idx = int(n_samples * (TRAIN_RATIO + VAL_RATIO))\n",
    "    \n",
    "    train_failures = y_turbine[:train_idx].sum()\n",
    "    val_failures = y_turbine[train_idx:val_idx].sum()\n",
    "    test_failures = y_turbine[val_idx:].sum()\n",
    "    \n",
    "    train_pct = (train_failures / train_idx * 100) if train_idx > 0 else 0\n",
    "    val_pct = (val_failures / (val_idx - train_idx) * 100) if (val_idx - train_idx) > 0 else 0\n",
    "    test_pct = (test_failures / (n_samples - val_idx) * 100) if (n_samples - val_idx) > 0 else 0\n",
    "    \n",
    "    marker = \"  <-- High Val\" if val_pct > 5 else \"\"\n",
    "    print(f\"T{turbine:<9} {train_pct:>8.2f}%  {val_pct:>8.2f}%  {test_pct:>8.2f}%{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84507b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard shuffling (WeightedRandomSampler disabled)\n",
      "DataLoaders: Train=125423, Val=25084, Test=58533\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_temp)\n",
    "y_train_tensor = torch.LongTensor(y_train_temp)\n",
    "X_val_tensor = torch.FloatTensor(X_val_temp)\n",
    "y_val_tensor = torch.LongTensor(y_val_temp)\n",
    "X_test_tensor = torch.FloatTensor(X_test_temp)\n",
    "y_test_tensor = torch.LongTensor(y_test_temp)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TimeSeriesDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TimeSeriesDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "class_counts = np.bincount(y_train_temp)\n",
    "\n",
    "# Guard against zero class counts\n",
    "if np.any(class_counts == 0):\n",
    "    print(\"WARNING: Training set missing one or more classes. Disabling WeightedRandomSampler.\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "elif USE_WEIGHTED_SAMPLER:\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = class_weights[y_train_temp]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "    print(f\"WeightedRandomSampler enabled with class distribution: {class_counts}\")\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(\"Using standard shuffling (WeightedRandomSampler disabled)\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dba08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none', weight=self.class_weights)(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abeda7",
   "metadata": {},
   "source": [
    "## 6. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcdab2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, num_classes=2):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=2, \n",
    "                           batch_first=True, dropout=DROPOUT, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a777a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels=[64, 128, 256], kernel_size=3, dropout=0.3, num_classes=2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_dim if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            padding = (kernel_size - 1) * dilation_size\n",
    "            layers += [nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                                 stride=1, padding=padding, dilation=dilation_size),\n",
    "                      nn.BatchNorm1d(out_channels),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(dropout)]\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.network(x)\n",
    "        x = x[:, :, -1]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd703ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=3, dropout=0.3, num_classes=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 1000, d_model))\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                     dim_feedforward=512, dropout=dropout, \n",
    "                                                     batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22938041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device='cuda', patience=10):\n",
    "    model.to(device)\n",
    "    best_val_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_precision': [], 'val_recall': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='binary', zero_division=0)\n",
    "        val_precision = precision_score(val_labels, val_preds, average='binary', zero_division=0)\n",
    "        val_recall = recall_score(val_labels, val_preds, average='binary', zero_division=0)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_without_improvement = 0\n",
    "            improvement_marker = \"*\"\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            improvement_marker = \"\"\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "              f\"F1: {val_f1:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} {improvement_marker}\")\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1} (best F1: {best_val_f1:.4f} at epoch {epoch+1-patience})\")\n",
    "            break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Model restored to best state (F1: {best_val_f1:.4f})\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ea1d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Guard ROC-AUC and PR-AUC computation against single-class splits\n",
    "    if len(np.unique(all_labels)) < 2:\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "    else:\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "        pr_auc = average_precision_score(all_labels, all_probs)\n",
    "    \n",
    "    return {\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'mcc': mcc,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2211bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_thresholds(model, test_loader, device='cuda', thresholds=[0.3, 0.5, 0.7]):\n",
    "    \"\"\"Evaluate model across multiple decision thresholds\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    results_per_threshold = {}\n",
    "    for thresh in thresholds:\n",
    "        preds = (all_probs >= thresh).astype(int)\n",
    "        results_per_threshold[thresh] = {\n",
    "            'precision': precision_score(all_labels, preds, average='binary', zero_division=0),\n",
    "            'recall': recall_score(all_labels, preds, average='binary', zero_division=0),\n",
    "            'f1': f1_score(all_labels, preds, average='binary', zero_division=0)\n",
    "        }\n",
    "    \n",
    "    return results_per_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1de6fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class weights enabled: Normal=0.5023, Failure=108.8741 (ratio=216.7x)\n",
      "\n",
      "Balancing strategy: Sampler=False, ClassWeights=True, FocalLoss=True\n",
      "Training CNN-LSTM...\n",
      "Epoch [  1/50] | Train Loss: 0.1029 | Val Loss: 6.4791 | F1: 0.1281 | Precision: 0.0885 | Recall: 0.2319 *\n",
      "Epoch [  2/50] | Train Loss: 0.0273 | Val Loss: 9.3756 | F1: 0.1249 | Precision: 0.0868 | Recall: 0.2222 \n",
      "Epoch [  3/50] | Train Loss: 0.0195 | Val Loss: 10.5137 | F1: 0.1328 | Precision: 0.1510 | Recall: 0.1186 *\n",
      "Epoch [  4/50] | Train Loss: 0.0178 | Val Loss: 11.8006 | F1: 0.1362 | Precision: 0.1382 | Recall: 0.1342 *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBalancing strategy: Sampler=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUSE_WEIGHTED_SAMPLER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ClassWeights=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUSE_CLASS_WEIGHTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, FocalLoss=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining CNN-LSTM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m model_cnn_lstm, history_cnn_lstm \u001b[38;5;241m=\u001b[39m train_model(model_cnn_lstm, train_loader, val_loader, \n\u001b[0;32m     24\u001b[0m                                                  criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS,\n\u001b[0;32m     25\u001b[0m                                                  device\u001b[38;5;241m=\u001b[39mdevice, patience\u001b[38;5;241m=\u001b[39mEARLY_STOPPING_PATIENCE)\n",
      "Cell \u001b[1;32mIn[49], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[45], line 9\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[1;32m----> 9\u001b[0m     ce_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights)(inputs, targets)\n\u001b[0;32m     10\u001b[0m     pt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mce_loss)\n\u001b[0;32m     11\u001b[0m     focal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pt) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m ce_loss\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32md:\\Softwares\\Anaconda\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "input_dim = X_pca.shape[1]\n",
    "seq_length = SEQ_LENGTH\n",
    "\n",
    "# Compute class weights if enabled\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_counts_train = np.bincount(y_train_temp)\n",
    "    class_weights_values = len(y_train_temp) / (len(class_counts_train) * class_counts_train)\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights_values).to(device)\n",
    "    print(f\"Class weights enabled: Normal={class_weights_values[0]:.4f}, Failure={class_weights_values[1]:.4f} (ratio={class_weights_values[1]/class_weights_values[0]:.1f}x)\")\n",
    "else:\n",
    "    class_weights_tensor = None\n",
    "    print(\"Class weights disabled\")\n",
    "\n",
    "model_cnn_lstm = CNN_LSTM(input_dim=input_dim, seq_length=seq_length)\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, class_weights=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_cnn_lstm.parameters(), lr=LR_CNN_LSTM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"\\nBalancing strategy: Sampler={USE_WEIGHTED_SAMPLER}, ClassWeights={USE_CLASS_WEIGHTS}, FocalLoss=True\")\n",
    "print(\"Training CNN-LSTM...\")\n",
    "model_cnn_lstm, history_cnn_lstm = train_model(model_cnn_lstm, train_loader, val_loader, \n",
    "                                                 criterion, optimizer, num_epochs=NUM_EPOCHS,\n",
    "                                                 device=device, patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b70a6",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10640838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save preprocessing objects\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "    \n",
    "with open('models/pca.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'RANDOM_SEED': RANDOM_SEED,\n",
    "    'WINDOW_HOURS': WINDOW_HOURS,\n",
    "    'PCA_VARIANCE': PCA_VARIANCE,\n",
    "    'SEQ_LENGTH': SEQ_LENGTH,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'NUM_EPOCHS': NUM_EPOCHS,\n",
    "    'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE,\n",
    "    'TRAIN_RATIO': TRAIN_RATIO,\n",
    "    'VAL_RATIO': VAL_RATIO,\n",
    "    'FOCAL_ALPHA': FOCAL_ALPHA,\n",
    "    'FOCAL_GAMMA': FOCAL_GAMMA,\n",
    "    'DROPOUT': DROPOUT,\n",
    "    'input_dim': input_dim,\n",
    "    'pca_components': X_pca.shape[1]\n",
    "}\n",
    "\n",
    "with open('models/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Preprocessing objects and configuration saved to models/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCNN-LSTM Test Set Results:\")\n",
    "results_cnn_lstm = evaluate_model(model_cnn_lstm, test_loader, device)\n",
    "print(f\"  F1-Score: {results_cnn_lstm['f1_score']:.4f}\")\n",
    "print(f\"  Precision: {results_cnn_lstm['precision']:.4f}\")\n",
    "print(f\"  Recall: {results_cnn_lstm['recall']:.4f}\")\n",
    "roc_str = f\"{results_cnn_lstm['roc_auc']:.4f}\" if not np.isnan(results_cnn_lstm['roc_auc']) else \"N/A\"\n",
    "pr_str = f\"{results_cnn_lstm['pr_auc']:.4f}\" if not np.isnan(results_cnn_lstm['pr_auc']) else \"N/A\"\n",
    "print(f\"  ROC-AUC: {roc_str}\")\n",
    "print(f\"  PR-AUC: {pr_str}\")\n",
    "print(f\"  MCC: {results_cnn_lstm['mcc']:.4f}\")\n",
    "print(f\"  Confusion Matrix:\\n{results_cnn_lstm['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dea30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CNN-LSTM model\n",
    "torch.save(model_cnn_lstm.state_dict(), 'models/cnn_lstm_best.pth')\n",
    "cnn_lstm_metrics = {\n",
    "    'f1_score': results_cnn_lstm['f1_score'],\n",
    "    'precision': results_cnn_lstm['precision'],\n",
    "    'recall': results_cnn_lstm['recall'],\n",
    "    'roc_auc': float(results_cnn_lstm['roc_auc']) if not np.isnan(results_cnn_lstm['roc_auc']) else None,\n",
    "    'pr_auc': float(results_cnn_lstm['pr_auc']) if not np.isnan(results_cnn_lstm['pr_auc']) else None,\n",
    "    'mcc': results_cnn_lstm['mcc']\n",
    "}\n",
    "with open('models/cnn_lstm_metrics.json', 'w') as f:\n",
    "    json.dump(cnn_lstm_metrics, f, indent=2)\n",
    "print(\"CNN-LSTM model saved to models/cnn_lstm_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c92dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThreshold Analysis (CNN-LSTM):\")\n",
    "threshold_results_cnn = evaluate_model_with_thresholds(model_cnn_lstm, test_loader, device)\n",
    "for thresh, metrics in threshold_results_cnn.items():\n",
    "    print(f\"  Threshold={thresh:.1f}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tcn = TCN(input_dim=input_dim, num_channels=TCN_CHANNELS, kernel_size=TCN_KERNEL_SIZE, dropout=DROPOUT)\n",
    "criterion_tcn = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, class_weights=class_weights_tensor)\n",
    "optimizer_tcn = optim.Adam(model_tcn.parameters(), lr=LR_TCN, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"\\nTraining TCN...\")\n",
    "model_tcn, history_tcn = train_model(model_tcn, train_loader, val_loader, \n",
    "                                      criterion_tcn, optimizer_tcn, num_epochs=NUM_EPOCHS,\n",
    "                                      device=device, patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTCN Test Set Results:\")\n",
    "results_tcn = evaluate_model(model_tcn, test_loader, device)\n",
    "print(f\"  F1-Score: {results_tcn['f1_score']:.4f}\")\n",
    "print(f\"  Precision: {results_tcn['precision']:.4f}\")\n",
    "print(f\"  Recall: {results_tcn['recall']:.4f}\")\n",
    "roc_str = f\"{results_tcn['roc_auc']:.4f}\" if not np.isnan(results_tcn['roc_auc']) else \"N/A\"\n",
    "pr_str = f\"{results_tcn['pr_auc']:.4f}\" if not np.isnan(results_tcn['pr_auc']) else \"N/A\"\n",
    "print(f\"  ROC-AUC: {roc_str}\")\n",
    "print(f\"  PR-AUC: {pr_str}\")\n",
    "print(f\"  MCC: {results_tcn['mcc']:.4f}\")\n",
    "print(f\"  Confusion Matrix:\\n{results_tcn['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceced159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TCN model\n",
    "torch.save(model_tcn.state_dict(), 'models/tcn_best.pth')\n",
    "tcn_metrics = {\n",
    "    'f1_score': results_tcn['f1_score'],\n",
    "    'precision': results_tcn['precision'],\n",
    "    'recall': results_tcn['recall'],\n",
    "    'roc_auc': float(results_tcn['roc_auc']) if not np.isnan(results_tcn['roc_auc']) else None,\n",
    "    'pr_auc': float(results_tcn['pr_auc']) if not np.isnan(results_tcn['pr_auc']) else None,\n",
    "    'mcc': results_tcn['mcc']\n",
    "}\n",
    "with open('models/tcn_metrics.json', 'w') as f:\n",
    "    json.dump(tcn_metrics, f, indent=2)\n",
    "print(\"TCN model saved to models/tcn_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ea808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = TransformerModel(input_dim=input_dim, d_model=TRANSFORMER_D_MODEL, nhead=TRANSFORMER_NHEAD, num_layers=TRANSFORMER_LAYERS, dropout=DROPOUT)\n",
    "criterion_transformer = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, class_weights=class_weights_tensor)\n",
    "optimizer_transformer = optim.Adam(model_transformer.parameters(), lr=LR_TRANSFORMER, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"\\nTraining Transformer...\")\n",
    "model_transformer, history_transformer = train_model(model_transformer, train_loader, val_loader, \n",
    "                                                       criterion_transformer, optimizer_transformer, num_epochs=NUM_EPOCHS,\n",
    "                                                       device=device, patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f113e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTransformer Test Set Results:\")\n",
    "results_transformer = evaluate_model(model_transformer, test_loader, device)\n",
    "print(f\"  F1-Score: {results_transformer['f1_score']:.4f}\")\n",
    "print(f\"  Precision: {results_transformer['precision']:.4f}\")\n",
    "print(f\"  Recall: {results_transformer['recall']:.4f}\")\n",
    "roc_str = f\"{results_transformer['roc_auc']:.4f}\" if not np.isnan(results_transformer['roc_auc']) else \"N/A\"\n",
    "pr_str = f\"{results_transformer['pr_auc']:.4f}\" if not np.isnan(results_transformer['pr_auc']) else \"N/A\"\n",
    "print(f\"  ROC-AUC: {roc_str}\")\n",
    "print(f\"  PR-AUC: {pr_str}\")\n",
    "print(f\"  MCC: {results_transformer['mcc']:.4f}\")\n",
    "print(f\"  Confusion Matrix:\\n{results_transformer['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39970aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Transformer model\n",
    "torch.save(model_transformer.state_dict(), 'models/transformer_best.pth')\n",
    "transformer_metrics = {\n",
    "    'f1_score': results_transformer['f1_score'],\n",
    "    'precision': results_transformer['precision'],\n",
    "    'recall': results_transformer['recall'],\n",
    "    'roc_auc': float(results_transformer['roc_auc']) if not np.isnan(results_transformer['roc_auc']) else None,\n",
    "    'pr_auc': float(results_transformer['pr_auc']) if not np.isnan(results_transformer['pr_auc']) else None,\n",
    "    'mcc': results_transformer['mcc']\n",
    "}\n",
    "with open('models/transformer_metrics.json', 'w') as f:\n",
    "    json.dump(transformer_metrics, f, indent=2)\n",
    "print(\"Transformer model saved to models/transformer_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models_results = {\n",
    "    'CNN-LSTM': results_cnn_lstm,\n",
    "    'TCN': results_tcn,\n",
    "    'Transformer': results_transformer\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(models_results.keys()),\n",
    "    'F1-Score': [r['f1_score'] for r in models_results.values()],\n",
    "    'Precision': [r['precision'] for r in models_results.values()],\n",
    "    'Recall': [r['recall'] for r in models_results.values()],\n",
    "    'ROC-AUC': [r['roc_auc'] for r in models_results.values()],\n",
    "    'PR-AUC': [r['pr_auc'] for r in models_results.values()],\n",
    "    'MCC': [r['mcc'] for r in models_results.values()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcb0cc",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "- All three architectures successfully trained on highly imbalanced data (< 1% failures)\n",
    "- Temporal split prevents data leakage while maintaining realistic evaluation\n",
    "- Triple balancing strategy (WeightedSampler + Focal Loss + Class Weights) enables failure detection\n",
    "\n",
    "### Performance Insights\n",
    "- **PR-AUC** is more informative than ROC-AUC for this imbalanced classification task\n",
    "- **MCC** provides balanced view of model performance across both classes\n",
    "- Precision-recall trade-off can be adjusted via decision threshold tuning\n",
    "\n",
    "### Limitations\n",
    "- PyTorch non-determinism may cause slight variation in results despite seed setting\n",
    "- Temporal split may result in different failure distributions across splits\n",
    "- Model performance depends heavily on WINDOW_HOURS parameter (48h currently)\n",
    "\n",
    "### Next Steps\n",
    "1. Hyperparameter tuning (WINDOW_HOURS, learning rates, architecture depth)\n",
    "2. Ensemble methods combining multiple architectures\n",
    "3. Attention visualization for interpretability\n",
    "4. Deployment with threshold optimization for production use\n",
    "5. Per-component failure prediction models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
